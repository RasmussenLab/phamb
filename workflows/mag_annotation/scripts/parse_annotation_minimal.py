#!/usr/bin/python

import argparse
import os
import sys
import gzip
import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from Bio import SeqIO

def parse_arguments():
    parser = argparse.ArgumentParser(description='')
    required = parser.add_argument_group('Required arguments')
    
    required.add_argument('-v',
                          '--vamb_directory',
                          dest='vamb_directory',
                          metavar='',
                          required=True,
                          type=str,
                          help='Path to file with all samples')
    required.add_argument('-s',
                          '--sample_table',
                          dest='sample_table',
                          metavar='',
                          required=True,
                          type=str,
                          help='Path to file with all samples')

    required.add_argument('-a',
                          '--annotation_dir',
                          dest='annotation_dir',
                          metavar='',
                          required=True,
                          type=str,
                          help='Path to Annotation directory with subdirs containing annotations of each sample') 
    required.add_argument('-o',
                          '--outdir',
                          dest='outdir',
                          metavar='',
                          required=True,
                          type=str,
                          help='Path to directory out.')

                    
    optional = parser.add_argument_group('Optional arguments')
    
    optional.add_argument('-d',
                          '--split_delimn',
                          dest='split_delimn',
                          metavar='',
                          default='_',
                          required=False,
                          type=str,
                          help='Character used to split Sample ID from contigname') 
    optional.add_argument('-m',
                          '--minimum_length',
                          dest='minimum_length',
                          metavar='',
                          default='2000',
                          required=False,
                          type=str,
                          help='Minimum contig length') 

    optional.add_argument('-g',
                          '--decontaminate',
                          dest='decontaminate',
                          action='store_true',
                          default=False,
                          help='Flag for running RF prediction of Viral Bins')
    optional.add_argument('-f',
                          '--fasta',
                          dest='fasta',
                          required=False,
                          default=None,
                          help='Same fasta used as input to VAMB')

    (args, extra_args) = parser.parse_known_args()

    return args



class bin_annotation:
    def __init__(self,bin_name,motherbin):
        self.motherbin = motherbin
        self.bin_name = bin_name
        self.ncontigs = None
        self.binsize = 0

### Initialising functions

def define_tasks(args):
    '''Checks the presence of files generated by the pipeline - files won't be created if already there
       User has to delete these files first.
    '''
    tasks = []

    ### Check for PVOG and MiComplete105
    for hmm in ['hmmMiComplete105','hmmVOG']:
        cleaned_hmmfile_out = os.path.join(args.outdir,hmm+'.allsamples.txt')
        if os.path.exists(cleaned_hmmfile_out):
            print('{} file already there - delete to generate again'.format(hmm))
        else:
            tasks.append(hmm)

    ### Check for DVF
    cleaned_DVF_out = os.path.join(args.outdir,'DVFpredictions.allsamples.txt')
    if os.path.exists(cleaned_DVF_out):
        print('DVF file already there - delete to generate again')
    else:
        tasks.append('DVF')

    ### Check for Prodigal
    allProdigal_out = os.path.join(args.outdir,'All.predicted_proteins.faa')
    allProdigal_out_gz =  os.path.join(args.outdir,'All.predicted_proteins.faa.gz')
    if os.path.exists(allProdigal_out) or os.path.exists(allProdigal_out_gz):
        print('Prodigal file already there - delete to generate again')
    else:
        tasks.append('Prodigal')
    

    ### Check if Bin-annotation tablee is already made 
    bintable = os.path.join(args.outdir,'vambbins_aggregated_annotation.txt')
    if os.path.exists(bintable):
        print('Bintable donee already - delete to generate again')
    else:
        tasks.append('Bintable')

    ### Run Random-Forrest Decontamination? 
    if args.decontaminate:
        tasks.append('Decontaminate')
    else:
        print('Not running Decontamination Mode (use --decontaminate flag for that.')

    return(tasks)


def parse_sample_table(args):
    '''Return list of samples'''
    samples = {}
    with open(args.sample_table,'r') as infile:
        for line in infile:
            line = line.strip().split()
            sample = line[0]
            samples[sample] = sample
    
    if len(samples) < 2:
        print('Ups! Are you missing samples in your sample_table.txt??? This method works best with >1 samples')
        sys.exit(1)

    return(samples)

def read_in_clusters(args):
    '''
    Parse VAMB cluster file - fits with VAMB v3 output.  
    This programme expects the structure to be: 

    binid\tcontigname
    '''
    cluster_file = os.path.join(args.vamb_directory,'clusters.tsv')
    contig_lengths_file = os.path.join('contig_lengths.npz')
    contig_file = os.path.join('contigs.npz')
    contigs = np.load(contig_file)['arr_0']
    contig_lengths = np.load(contig_lengths_file)['arr_0']
    contig_length_lookup = {contigs[i]:contig_lengths[i] for i in range(len(contigs))}
    print('Reading in Clusters, this may take a while...')
    binsannos = dict()
    cls = dict()
    with open(cluster_file,'r') as infile:
        for line in infile:
            line = line.strip().split('\t')
            binid, contig = line[0],line[1]
            sample, cluster = binid.split('C')
            contiglength = contig_length_lookup[contig]
            if binid not in binsannos:
                binsannos[binid] = bin_annotation(binid,cluster)
                binsannos[binid].ncontigs = 1
                binsannos[binid].binsize += contiglength
            else:
                binsannos[binid].ncontigs += 1
                binsannos[binid].binsize += contiglength

            if contig not in cls:
                cls[contig] = (cluster,binid)
    return cls, binsannos



                
############### For HMMfiles ###############
def parse_hmmfiles(args,task_list,samples):
    '''Generic function filter and parse files generated with hmmsearch'''
    
    for hmm in ['hmmMiComplete105','hmmVOG']:
        if hmm not in task_list:
            continue
        cleaned_hmmfile_out = os.path.join(args.outdir,hmm+'.allsamples.txt')
        with open(cleaned_hmmfile_out,'w') as outfile:
            for sample in samples:
                hmmfile = os.path.join(args.annotation_dir,sample,sample+'.'+hmm+'.tbl') 
                if not os.path.isfile(hmmfile):
                    print('{} does not exist for sample: {}'.format(hmmfile,sample))
                    continue
                for string in hmm_outline_generator(hmmfile,sample,args.split_delimn):
                    outfile.write(string)

def hmm_outline_generator(hmmfile,sample,seperator='_'):
    with open(hmmfile,'r') as infile:
        target_protein = dict()
        for line in infile:
            if line[0] != '#':
                entry = line.strip().split()
                if len(entry) > 8:
                    protein, target, full_seq_eval , full_seq_score , best_dom_eval, best_dom_score =  entry[0], entry[2], entry[4], entry[5], entry[7],entry[8]
                    tmp = protein.split(seperator)
                    genome_contig = seperator.join(tmp[:-1])
                    full_seq_score = float(full_seq_score)
                    if float(full_seq_score) <= 50:
                        continue
                    if protein not in target_protein:
                        target_protein[protein] = []
                        target_protein[protein] += [(full_seq_score,genome_contig,protein,target,full_seq_eval)]
                    else:
                        target_protein[protein] += [(full_seq_score,genome_contig,protein,target,full_seq_eval)]
    for protein in target_protein:
        protein_targets = target_protein[protein]
        
        ### sort by domain score 
        protein_targets = sorted(protein_targets, key=lambda x: x[0] ,reverse=True)
        full_seq_score , contig, ORF, target, full_seq_eval = protein_targets[0]
        subentry = [contig, ORF, target, full_seq_score, full_seq_eval]
        subentry = [str(item) for item in subentry]
        string = '\t'.join(subentry) + '\n'
        yield string


############### For DVF files ############### 
def parse_DVF(args,task_list,samples):
    '''Generic function filter and parse files generated by DVF 
    '''
    if 'DVF' not in task_list:
        pass
    else:
        cleaned_DVF_out = os.path.join(args.outdir,'DVFpredictions.allsamples.txt')
        with open(cleaned_DVF_out,'w') as outfile:
            for sample in samples:
                #dvffile = os.path.join(args.annotation_dir,sample,sample + '_dvf',sample + '.flt.fa_gt' + args.minimum_length + 'bp_dvfpred.txt')
                dvfdir = os.path.join(args.annotation_dir,sample,sample + '_dvf')
                dvffiles = [f for f in os.listdir(dvfdir) if os.path.isfile(os.path.join(dvfdir, f)) and 'dvfpred' in f]
                if len(dvffiles) == 0:
                    print('DVF does not exist for sample: {}'.format(sample))
                    continue
                dvffile = os.path.join(args.annotation_dir,sample,sample + '_dvf',dvffiles[0])
                for string in DVF_generator(dvffile,args.split_delimn):
                    outfile.write(string)

def DVF_generator(dvffile,seperator):
    with open(dvffile,'r') as infile:
        for line in infile:
            if line[:4] =='name':
                continue
            line = line.strip().split('\t')
            if not len(line) == 4:
                print(dvffile,'WARNING: this file might be corrupted!')
                continue
            contig,score,pval = line[0],line[2],line[3]
            sample = contig.split(seperator)[0]
            dvf_entry = [contig,score,pval,sample]
            string = '\t'.join(dvf_entry) + '\n'
            yield string

############### For Predicted Proteins ############### 

def parse_Prodigal(args,task_list,samples):
    '''Generic function to concatenate Predicted Proteins by Prodigal 
    '''
    if 'Prodigal' not in task_list:
        pass
    else:
        allProdigal_out = os.path.join(args.outdir,'All.predicted_proteins.faa')
        with open(allProdigal_out,'w') as outfile:
            for sample in samples:
                Prodigalfile = os.path.join(args.annotation_dir,sample,sample + '.predicted_proteins.faa')
                if not os.path.isfile(Prodigalfile):
                    print('{} does not exist for sample: {}'.format(Prodigalfile,sample))
                    continue
                for string in Prodigal_generator(Prodigalfile):
                    outfile.write(string)

def Prodigal_generator(Prodigalfile):
    '''There is actually not much to do but concatenate the lines of Prodigal files
       This generator can be expanded to also accomodate filters and header modifications.
    '''
    with open(Prodigalfile,'r') as infile:
        for line in infile:
            if line[0] != '#':
                yield line


############### Summarise annotation for Bins ###############

def parse_bacterial_hallmarks(args, cls):
    '''
    Count the number of Distinct Bacterial Hallmark Genes in each Cluster
    '''
    hallmark_file = os.path.join(args.outdir,'hmmMiComplete105.allsamples.txt')
    cluster_hallmarks = dict()
    with open(hallmark_file,'r') as infile:
        for line in infile:
            contig, protein, hallmark_gene, score, evalue = line.strip().split('\t')

            if contig in cls:
                cluster = cls[contig][0]

                if cluster not in cluster_hallmarks:
                    cluster_hallmarks[cluster] = set([hallmark_gene])
                else:
                    cluster_hallmarks[cluster].add(hallmark_gene)
    cluster_hallmark_count = dict()
    for cluster in cluster_hallmarks:
        n_distinct_hallmarks = len(cluster_hallmarks[cluster])
        cluster_hallmark_count[cluster] = n_distinct_hallmarks
    return cluster_hallmark_count

def parse_PVOG_hits(args,cls,binsannos,hmmscore = 50):
    '''
    Count the number of distinct VOG annotations scaled by ncontigs total
    '''
    vogfile = os.path.join(args.outdir,'hmmVOG.allsamples.txt')
    if not os.path.exists(vogfile):
        print('Missing VOG file !')
        sys.exit(1)
    bin_vogs = dict()
    with open(vogfile,'r') as infile:
        for line in infile:
            contig, protein, vog, score, evalue = line.strip().split('\t')
            if float(score) <= hmmscore:
                continue

            if contig in cls:
                binid = cls[contig][1]
                if binid not in bin_vogs:
                    bin_vogs[binid] = set([vog])
                else:
                    bin_vogs[binid].add(vog)
    
    ### Summarise VOGs for each bin 
    bin_vog_count = dict()
    for binid in bin_vogs:
        n_contigs = binsannos[binid].ncontigs
        vog_count_scaled = len(bin_vogs[binid])/n_contigs
        bin_vog_count[binid] = vog_count_scaled
    return bin_vog_count


        
def parse_DVF_preds(args,cls):
    '''
    Calculate median DVF score for each bin and clustre
    '''

    dvf_file = os.path.join(args.outdir,'DVFpredictions.allsamples.txt')
    if not os.path.exists(dvf_file):
        print('Missing DVF file !')
        sys.exit(1)

    cluster_dvf_scores = dict()
    bin_dvf_scores = dict()
    with open(dvf_file,'r') as infile:
        for line in infile:
            contig, score, pvalue, sample = line.strip().split('\t')

            if contig in cls:
                score = float(score)
                binid = cls[contig][1]
                cluster = cls[contig][0]
                if cluster not in cluster_dvf_scores:
                    cluster_dvf_scores[cluster] = [score]
                else:
                    cluster_dvf_scores[cluster] = [score]

                if binid not in bin_dvf_scores:
                    bin_dvf_scores[binid] = [score]
                else:
                    bin_dvf_scores[binid] += [score]
    
    ### calculate median DVF score pr. bin 
    bin_median_dvf = dict()
    for binid in bin_dvf_scores:
        median_score = np.median(bin_dvf_scores[binid])
        bin_median_dvf[binid] = median_score

    ### Same for Cluster
    cluster_median_dvf = dict()
    for cluster in cluster_dvf_scores:
        median_score = np.median(cluster_dvf_scores[cluster])
        cluster_median_dvf[cluster] = median_score
    
    return bin_median_dvf, cluster_median_dvf


def aggregate_bin_annotation(args,cls,binsannos):


    print('Parsing Bacterial Hallmarks')
    cluster_hallmark_count = parse_bacterial_hallmarks(args, cls)

    print('Parsing VOG HMM search')
    bin_vog_count = parse_PVOG_hits(args,cls,binsannos)

    print('Parsing DVF contig scores - this may take some timee...')
    bin_median_dvf, cluster_median_dvf = parse_DVF_preds(args,cls)

    ### Write out table with aggregated information for each Bin
    fileout = os.path.join(args.outdir,'vambbins_aggregated_annotation.txt')

    if os.path.exists(fileout):
        print(fileout,' allready exists, delete it to reproduce it!')
        sys.exit(0)

    print('Writing results to:',fileout)
    with open(fileout,'w') as out:
        header = ['binid','ncontigs','binsize','motherbin','nhallm','nVOGs','bin_DVF_score','cluster_DVF_score','annotation_label']
        out.write('\t'.join(header) + '\n')
        for binid in binsannos:
            lineout = []
            lineout.append(binid)

            ### add bin annotation
            ncontigs = binsannos[binid].ncontigs
            binsize = binsannos[binid].binsize

            ### Going this low...
            if binsize <= 2500:
                continue
            mothercluster = binsannos[binid].motherbin
            if mothercluster not in cluster_hallmark_count:
                nhallmarks = 0
            else:
                nhallmarks = cluster_hallmark_count[mothercluster]
            if binid not in bin_vog_count:
                nVOGs = 0
            else:
                nVOGs = bin_vog_count[binid]
                nVOGs = round(nVOGs,2)
            if binid not in bin_median_dvf:
                bin_DVF_score = 0
            else:
                bin_DVF_score = bin_median_dvf[binid]
                bin_DVF_score = round(bin_DVF_score,2)
            if mothercluster not in cluster_median_dvf:
                cluster_DVF_score = 0
            else:
                cluster_DVF_score = cluster_median_dvf[mothercluster]
                cluster_DVF_score = round(cluster_DVF_score,2)

            ### 
            if all( i == 0 for i in [nhallmarks, nVOGs, bin_DVF_score, cluster_DVF_score] ) is True:
                annotation_label = 'not-annotated'
            else:
                annotation_label = 'annotated'
            lineout += [ncontigs,binsize, mothercluster, nhallmarks, nVOGs,bin_DVF_score,cluster_DVF_score,annotation_label]
            lineout = [str(x) for x in lineout]
            out.write('\t'.join(lineout) + '\n')



def write_concatenated_sequences(args,cls,_vambbins_filtered):
    '''
    Write out Contigs Concatenated as Bins 
    Write out Contigs individually
    '''
    viral_contigs_out = os.path.join(args.outdir,'VAMB.Viral_RF_predictions.contigs.fna')
    viral_bins_out = os.path.join(args.outdir,'VAMB.Viral_RF_predictions.bins.fna')
    binids = set(_vambbins_filtered.binid)
    print('Parsing Fasta sequences - this may take a while...')
    clusters = {}
    with open(viral_contigs_out,'w') as out:
        for record in SeqIO.parse(gzip.open(args.fasta, 'rt'), 'fasta'):
            contigname = record.id 
            if contigname not in cls:
                continue
            binid = cls[contigname][1]
            if binid in binids:
                out.write('>{}\n{}\n'.format(contigname,record.seq))
                if binid not in clusters:
                    clusters[binid] = ""
                    clusters[binid] += record.seq
                else:
                    clusters[binid] += record.seq
    
    ### Write out concatenated bins
    with open(viral_bins_out,'w') as out:
        for binid in clusters:
            sequence = clusters[binid]
            out.write('>{}\n{}\n'.format(binid,sequence))


def RF_decontaminate(args):
    '''
    - Load table with bin-summarised information , size , PVOG proteins and bacterial hallmarks etc.
    - Predict Viral and Bacterial bins using the Random Forest model 
    '''

    table_file = os.path.join(args.outdir,'vambbins_aggregated_annotation.txt')
    if not os.path.exists(table_file):
        print('Ups! Where is your VAMB bins file?:',table_file)
        sys.exit(1)

    print('Loading Model and annotation table')
    trained_model = joblib.load('mag_annotation/dbs/RF_model.sav')
    _vambbins = pd.read_csv(table_file,sep='\t')
    _vambbins = _vambbins[ _vambbins.annotation_label == 'annotated']

    _subset = _vambbins[['binsize','nhallm','nVOGs','cluster_DVF_score']]
    eval_predictions = trained_model.predict(_subset)
    prediction_labels = eval_predictions
    _vambbins['Prediction'] = prediction_labels
    _vambbins_filtered = _vambbins[ (_vambbins.Prediction =='Viral') ]
    
    table_file_annotated = os.path.join(args.outdir,'vambbins_aggregated_annotation.Viral.txt')
    _vambbins_filtered.sort_values(by='binsize',ascending=False).to_csv(table_file_annotated,sep='\t',index=False)

    ### Just in case, read clusters from VAMB clusters file.
    cls, binsannos = read_in_clusters(args)

    if args.fasta is None:
        print('You need to provide the -f argument in order to write fasta files of Viral Bins and Contigs!')
        sys.exit(0)
    ### Write out contigs and Bins concatenated
    write_concatenated_sequences(args,cls,_vambbins_filtered)
    print('Done Writing Viral Sequences!')



############### Main ############### 
if __name__ == "__main__":
    args = parse_arguments()

    ### Get sample names
    samples = parse_sample_table(args)

    ### Define tasks 
    task_list = define_tasks(args)

    if len(task_list) == 0:
        print('All expected outfiles already exist in {} - We are done here'.format(args.outdir))
        sys.exit(0)
    try:
        os.mkdir(args.outdir)
    except:
        pass

    ### Run parsers
    parse_hmmfiles(args,task_list,samples)
    parse_DVF(args,task_list,samples)
    parse_Prodigal(args,task_list,samples)
    print('Done')
    if 'Bintable' in task_list:

        ### Read in VAMB cluster information
        cls, binsannos = read_in_clusters(args)

        ### Calculate and summarise Bin-information
        aggregate_bin_annotation(args,cls,binsannos)
    
    if 'Decontaminate' in task_list:
        print('Running RF - decontaminate')
        RF_decontaminate(args)

